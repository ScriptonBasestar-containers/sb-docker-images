# Jupyter Standalone Environment Configuration
# Copy this file to .env and customize the values

# ====================
# Project Settings
# ====================
PROJECT_NAME=jupyter
TZ=Asia/Seoul

# ====================
# Port Configuration
# ====================
# Jupyter Notebook web UI port
JUPYTER_PORT=8888

# ====================
# Jupyter Configuration
# ====================
# Jupyter access token (⚠️ CHANGE THIS!)
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
JUPYTER_TOKEN=change-me-secure-token

# Enable Jupyter Lab instead of classic Notebook
JUPYTER_ENABLE_LAB=yes

# ====================
# User Configuration
# ====================
# Jupyter user (default: jovyan)
NB_USER=jovyan

# User ID (match with host user for file permissions)
NB_UID=1000

# Group ID
NB_GID=100

# Grant sudo privileges (use with caution)
GRANT_SUDO=no

# ====================
# Docker Image
# ====================
# Image name (custom-built)
JUPYTER_IMAGE_NAME=jupyter-fullbook
JUPYTER_IMAGE_TAG=latest

# ====================
# Container Names
# ====================
JUPYTER_CONTAINER_NAME=jupyter-notebook

# ====================
# Network Names
# ====================
JUPYTER_NETWORK=jupyter-network

# ====================
# Volume Names
# ====================
JUPYTER_WORK_VOLUME=jupyter-work
JUPYTER_CONFIG_VOLUME=jupyter-config

# ====================
# About Jupyter Full Stack
# ====================
# This Jupyter environment includes:
# - Python 3 with full data science stack
# - R with IRKernel and tidyverse
# - Julia with IJulia
# - TensorFlow for deep learning
# - Apache Spark for big data
#
# Pre-installed libraries:
# - NumPy, Pandas, Matplotlib, Seaborn
# - Scikit-learn, SciPy, Statsmodels
# - Beautiful Soup, SQLAlchemy
# - Dask, H5py, HDF5
# - Bokeh, ipywidgets

# ====================
# Quick Start
# ====================
# 1. Build image:
#    make build
#
# 2. Start Jupyter:
#    make up
#
# 3. Get access URL:
#    make token
#
# 4. Open browser:
#    http://localhost:8888
#
# 5. Enter token from step 3

# ====================
# Available Kernels
# ====================
# Check installed kernels:
#   make kernels
#
# Available kernels:
# - Python 3: Default data science kernel
# - R (IRKernel): Statistical computing
# - Julia: High-performance scientific computing
# - Spark (PySpark): Big data processing

# ====================
# Working with Notebooks
# ====================
# Notebooks are stored in Docker volume: jupyter-work
#
# To access notebooks from host:
# 1. Copy from container:
#    docker cp jupyter-notebook:/home/jovyan/work ./notebooks
#
# 2. Or mount local directory (modify compose.yml):
#    volumes:
#      - ./notebooks:/home/jovyan/work
#
# 3. Backup notebooks:
#    make shell
#    tar czf /tmp/notebooks-backup.tar.gz work/
#    exit
#    docker cp jupyter-notebook:/tmp/notebooks-backup.tar.gz .

# ====================
# Installing Additional Packages
# ====================
# Python packages:
#   # Temporary (inside notebook):
#   !pip install package-name
#
#   # Persistent (as root):
#   make shell
#   pip install package-name
#
# R packages:
#   # Inside R kernel:
#   install.packages("package-name")
#
# Julia packages:
#   # Inside Julia kernel:
#   using Pkg
#   Pkg.add("PackageName")
#
# System packages (requires root):
#   make shell
#   apt-get update
#   apt-get install package-name

# ====================
# Usage Examples
# ====================
# Python - Data Analysis:
#   import pandas as pd
#   import matplotlib.pyplot as plt
#   df = pd.read_csv('data.csv')
#   df.plot()
#   plt.show()
#
# R - Statistical Analysis:
#   library(tidyverse)
#   data <- read.csv('data.csv')
#   ggplot(data, aes(x=x, y=y)) + geom_point()
#
# Julia - Numerical Computing:
#   using DataFrames, Plots
#   df = DataFrame(x = 1:10, y = rand(10))
#   plot(df.x, df.y)
#
# TensorFlow - Deep Learning:
#   import tensorflow as tf
#   model = tf.keras.Sequential([
#       tf.keras.layers.Dense(64, activation='relu'),
#       tf.keras.layers.Dense(10, activation='softmax')
#   ])
#
# Spark - Big Data:
#   from pyspark.sql import SparkSession
#   spark = SparkSession.builder.appName("MyApp").getOrCreate()
#   df = spark.read.csv('data.csv', header=True)
#   df.show()

# ====================
# Security Notes
# ====================
# ⚠️  IMPORTANT Security Recommendations:
#
# 1. Authentication:
#    - Always use a strong JUPYTER_TOKEN
#    - Generate secure token: python -c "import secrets; print(secrets.token_urlsafe(32))"
#    - Never use empty token in production
#
# 2. Network Security:
#    - Only expose on localhost for development
#    - Use reverse proxy with HTTPS for production
#    - Consider using VPN or SSH tunnel for remote access
#
# 3. File Permissions:
#    - Match NB_UID with your host user ID
#    - Set appropriate permissions on mounted volumes
#    - Don't run as root unless necessary (GRANT_SUDO=no)
#
# 4. Data Protection:
#    - Backup notebooks regularly
#    - Use version control (Git) for notebooks
#    - Encrypt sensitive data
#    - Don't commit credentials to notebooks
#
# 5. Resource Limits:
#    - Set memory limits to prevent system crashes
#    - Monitor resource usage: docker stats
#    - Close unused notebooks
#
# 6. Updates:
#    - Rebuild image regularly for security patches
#    - Update Python packages: pip install --upgrade package
#    - Update system: apt-get update && apt-get upgrade

# ====================
# Performance Tuning
# ====================
# For large datasets or heavy computations:
#
# 1. Increase memory (in compose.yml):
#    deploy:
#      resources:
#        limits:
#          memory: 8G
#
# 2. Use multiple CPU cores (in compose.yml):
#    deploy:
#      resources:
#        limits:
#          cpus: '4'
#
# 3. Enable GPU support (NVIDIA):
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [gpu]
#
# 4. Optimize Jupyter:
#    - Close unused notebooks
#    - Clear output before saving
#    - Use %matplotlib inline for plots
#    - Restart kernel to free memory

# ====================
# JupyterLab vs Notebook
# ====================
# JupyterLab (JUPYTER_ENABLE_LAB=yes):
# - Modern interface
# - Multiple tabs and panes
# - Integrated file browser
# - Extensions support
# - Terminal access
#
# Classic Notebook (JUPYTER_ENABLE_LAB=no):
# - Simple interface
# - Single notebook view
# - Lightweight
# - Better for simple tasks

# ====================
# Exporting Notebooks
# ====================
# Export as HTML:
#   File > Download as > HTML
#
# Export as Python:
#   File > Download as > Python (.py)
#
# Export as PDF (requires LaTeX):
#   File > Download as > PDF via LaTeX
#
# Share on GitHub:
#   GitHub natively renders .ipynb files
#
# Use nbviewer:
#   https://nbviewer.org/

# ====================
# Troubleshooting
# ====================
# Cannot access Jupyter:
#   - Check if container is running: make ps
#   - Get access token: make token
#   - Check logs: make logs
#
# Kernel not starting:
#   - Check logs: make logs
#   - Restart Jupyter: make restart
#   - List kernels: make kernels
#
# Out of memory:
#   - Close unused notebooks
#   - Restart kernel
#   - Increase memory limits
#   - Check: docker stats jupyter-notebook
#
# Permission denied:
#   - Match NB_UID with host user
#   - Check volume permissions
#   - Run as root if needed: GRANT_SUDO=yes
#
# Package import error:
#   - Restart kernel
#   - Reinstall package: make shell; pip install package
#   - Check kernel: make kernels

# ====================
# Monitoring Commands
# ====================
# View logs:
#   make logs
#
# Check resource usage:
#   docker stats jupyter-notebook
#
# List running notebooks:
#   make token
#
# Check kernels:
#   make kernels
#
# Access shell:
#   make shell

# ====================
# References
# ====================
# - Jupyter: https://jupyter.org/
# - JupyterLab: https://jupyterlab.readthedocs.io/
# - Jupyter Notebook: https://jupyter-notebook.readthedocs.io/
# - NumPy: https://numpy.org/doc/
# - Pandas: https://pandas.pydata.org/docs/
# - Matplotlib: https://matplotlib.org/
# - Scikit-learn: https://scikit-learn.org/
# - TensorFlow: https://www.tensorflow.org/
# - Spark: https://spark.apache.org/docs/latest/
# - Port guide: ../../PORT_GUIDE.md
